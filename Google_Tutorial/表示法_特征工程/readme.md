https://developers.google.com/machine-learning/crash-course/representation/qualities-of-good-features
几个知识点：
1、特征工程：原始数据->特征矢量
2、one-hot ：记得把一些string转化，记得这个ont-hot点，其实见到很多次了
3、好的特征：要多次出现，至少5次
           最好有明确的意义
           一些有奇怪数据的特征可以转化为boolean
           特征的定义不应随时间发生变化？？这个待定吧，有的数据可能是和时间有关的

4、数据清理：
        缩放特征值（归一化）scalevalue=(value-mean)/stddev 变成标准正太
缩放是指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。如果某个特征集只包含一个特征，则缩放可以提供的实际好处微乎其微或根本没有。
不过，如果特征集包含多个特征，则缩放特征可以带来以下优势：
帮助梯度下降法更快速地收敛。
帮助避免“NaN 陷阱”。在这种陷阱中，模型中的一个数值变成 NaN（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 NaN。
帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力。
您不需要对每个浮点特征进行完全相同的缩放。即使特征 A 的范围是 -1 到 +1，同时特征 B 的范围是 -3 到 +3，也不会产生什么恶劣的影响。不过，如果特征 B 的范围是 5000 到 100000，您的模型会出现糟糕的响应。

5、 处理极端离群值
                 1、取log:v=log(value+1)
                 2、做个限制。使用min() max() 等
6、分箱 这个现在还没怎么见过
7、清查 数据中有些不好的：直方图、max()、min()、mean、average、stddev

